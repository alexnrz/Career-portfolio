{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extract_reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPgc6Xu48uaZksi5cx9VZPx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\"\"\"\n","Created on Mon Oct 2021\n","\n","@author: Alex Nascimento Rodrigues\n","\"\"\""],"metadata":{"id":"H3cP6tQ_vkNk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKx0UGWgvX3r"},"outputs":[],"source":["from typing import Counter\n","from selenium import webdriver\n","import bs4\n","import os\n","import json\n","from time import sleep  \n","\n","### Incializando webdriver ###\n","def web_driver_creator():     \n","  chrome_options = webdriver.ChromeOptions()\n","  chrome_options.add_argument('--headless')\n","  chrome_options.add_argument('--log-level=1')\n","  chrome_options.add_argument('--no-sandbox')\n","  chrome_options.add_argument('--disable-dev-shm-usage')\n","  wd = webdriver.Chrome('chromedriver',options=chrome_options)\n","  \n","  return wd\n","\n","### Gerando a url inicial e posteriores ###\n","def generate_url(search_term, page):    \n","  departamento = ''\n","      \n","  if search_term == 'livros':\n","      departamento = 'stripbooks'\n","  elif search_term == 'eletronicos':\n","      departamento = 'electronics'\n","  elif search_term == 'brinquedos':\n","      departamento = 'toys'\n","  \n","  \"\"\"Generate a url from seacrh term\"\"\"\n","  base_template = 'https://www.amazon.com.br/s?k={}&i={}&page={}&ref=sr_pg_{}'\n","  search_term = search_term.replace(' ', '')\n","  url = base_template.format(search_term,departamento,page,page)\n","  \n","  return url\n","    \n","### Retirando os dados da primeira página ###\n","def extract_record(item):    \n","  try:\n","    # Nome do objeto\n","    target_name = item.h2.a.text.strip()    \n","    # url de cada objeto\n","    url_asin = item.get('data-asin')\n","      \n","  except AttributeError:\n","    return\n","  \n","  required_info = (target_name, url_asin)  \n","  return required_info\n","\n","def target_and_urls (search_term, page_number):\n","    \n","  wd = web_driver_creator()\n","  search_url = generate_url(search_term, page_number)\n","  print(search_url)        \n","  wd.get(search_url)        \n","  soup = bs4.BeautifulSoup(wd.page_source, 'html.parser')\n","  \n","  target_data_asin = []\n","  results = soup.find_all('div', {'data-component-type' : 's-search-result'})\n","      \n","  # record = [Nome do objeto, url]\n","  for item in results:\n","    record = extract_record(item)\n","    if record:\n","      target_data_asin.append(record)\n","              \n","  wd.quit()  \n","  return(target_data_asin)\n","\n","def get_text_stars_reviews (target_code,num_max_reviews):\n","    \n","  wd = web_driver_creator()\n","  reviews_and_stars = []\n","  all_user_names = []    \n","  all_stars = []\n","  all_dates = []\n","  all_reviews = []\n","  \n","  # 10 Reviews por página\n","  number_pages = (int(num_max_reviews) / 10) + 1\n","  \n","  # implementar a verificação quando não houver mais reviews\n","  \n","  for page in range(1,int(number_pages)):      \n","    base_url = 'https://www.amazon.com.br/product-reviews/{}/ref=cm_cr_getr_d_paging_btm_next_{}?pageNumber={}'.format(target_code,page,page)\n","    \n","    wd.get(base_url)\n","    soup = bs4.BeautifulSoup(wd.page_source, 'html.parser')     \n","    all_user_names = soup.find_all('span', {'class' : 'a-profile-name'})    \n","    all_reviews = soup.find_all('span', {'data-hook' : 'review-body'})    \n","    all_dates = soup.find_all('span', {'class' : 'a-size-base a-color-secondary review-date'})    \n","    all_stars = soup.find_all('i', {'data-hook' : 'review-star-rating'})\n","      \n","    index = 0      \n","    for index in range (0, len(all_stars)):\n","      try: \n","        reviews_and_stars.append([all_user_names[index].text,\n","                                all_stars[index].text.replace('de 5 estrelas','').strip(),\n","                                all_dates[index].text.replace('Avaliado no Brasil em','').strip(),\n","                                all_reviews[index].text.replace('Your browser does not support HTML5 video.','').strip()])\n","\n","      except AttributeError:\n","        return\n","    sleep(3)\n","      \n","  wd.quit()\n","  return reviews_and_stars\n","\n","def extract_empty_items(num_max_reviews, num_max_targets,\n","                        file_existing_items, results_file, target_name):\n","  dicio = {}\n","  list_collected_reviews = []\n","  results = []\n","  index_max_targets = 0\n","  index_list_target = 0\n","  page = 1\n","      \n","  results = target_and_urls(target_name, page)\n","          \n","  list_target_names = []\n","  list_target_names = [item[0] for item in results]\n","  list_target_codes = []\n","  list_target_codes = [item[1] for item in results]\n","  \n","  for index_max_targets in range(0,num_max_targets):\n","    # A cada 22 items coletados uma nova página é carregada\n","    if (index_max_targets % 22 == 0 and index_max_targets != 0):            \n","      page += 1\n","      \n","      try:\n","        results = target_and_urls(target_name, page)\n","        \n","        list_target_names = []\n","        list_target_names = [item[0] for item in results]\n","        list_target_codes = []\n","        list_target_codes = [item[1] for item in results]\n","        print(\"--- Coletando na página:\",page, \"---\")\n","        index_list_target = 0\n","        sleep(3)\n","      except:\n","        print(\"Erro na coleta de nomes e códigos!\")\n","        break\n","    \n","    print(\"Coletando\", index_max_targets + 1, \"de\", num_max_targets)\n","    \n","    try:\n","        with open(results_file, 'r+', encoding='utf8') as json_file:\n","          dicio[list_target_names[index_list_target]] = get_text_stars_reviews(list_target_codes[index_list_target],num_max_reviews)\n","          if (index_max_targets == 0):\n","            data = {}\n","          else:\n","            data = json.load(json_file)      \n","              \n","          data.update(dicio)\n","          json_file.seek(0)\n","          json.dump(data, json_file, ensure_ascii=False)                                             \n","          list_collected_reviews.append(list_target_names[index_list_target])\n","    \n","        index_list_target += 1\n","        dicio = {}\n","        \n","    except:\n","      print(\"Erro na coleta de nomes e códigos!\")\n","      break\n","  \n","  with open(file_existing_items, 'w', encoding='utf8') as json_file:\n","    json.dump(list_collected_reviews, json_file, ensure_ascii=False)\n","    \n","def extract_existing_items(num_max_reviews, num_max_targets,\n","                           file_existing_items, results_file, target_name):\n","  dicio = {}\n","  parsed_json = []\n","  \n","  # Contém os livros que já foram coletados\n","  with open(file_existing_items, 'r', encoding='utf8') as json_file:\n","    parsed_json = json.load(json_file)\n","    json_file.close()                \n","\n","  # Uso do set para diminuir a complexidade\n","  set_target = set(parsed_json)\n","  \n","  page = 1\n","  results = target_and_urls(target_name, page)\n","  \n","  list_target_names = []\n","  list_target_names = [item[0] for item in results]\n","  \n","  list_target_codes = []\n","  list_target_codes = [item[1] for item in results]\n","  \n","  index_max_targets = 0\n","  index_list_target = 0\n","  existing_cont = num_max_targets\n","  new_item_cont = 1\n","      \n","  while index_max_targets < num_max_targets:\n","  # A cada 22 items coletados uma nova página é carregada\n","    if (index_max_targets % 22 == 0 and index_max_targets != 0):\n","      page += 1\n","      try:\n","        results = target_and_urls(target_name, page)\n","        list_target_names = []\n","        list_target_names = [item[0] for item in results]\n","        \n","        list_target_codes = []\n","        list_target_codes = [item[1] for item in results]\n","        print(\"Coletando na página:\",page)\n","        index_list_target = 0\n","        sleep(3)\n","      except:\n","        print(\"Erro na coleta de nomes e códigos!\")\n","        break\n","    \n","    print(\"Coletando\", new_item_cont, \"de\", existing_cont)\n","    \n","    try:\n","      if list_target_names[index_list_target] not in set_target:\n","        with open(results_file, 'r+', encoding='utf8') as json_file:\n","          dicio[list_target_names[index_list_target]] = get_text_stars_reviews(list_target_codes[index_list_target],num_max_reviews)\n","          data = json.load(json_file)\n","          data.update(dicio)\n","          json_file.seek(0)\n","          json.dump(data, json_file, ensure_ascii=False)\n","          json_file.close()\n","                \n","        set_target.add(list_target_names[index_list_target])            \n","        \n","        new_item_cont += 1\n","        dicio = {}\n","      else:\n","        print(\"O item ->\",list_target_names[index_list_target],\"<- já foi coletado\")\n","        num_max_targets += 1\n","  \n","      index_max_targets += 1\n","      index_list_target += 1\n","    except:\n","      print(\"Erro na coleta de reviews!\")\n","      break\n","  \n","  with open(file_existing_items, 'w', encoding='utf8') as json_file:\n","    json.dump(list(set_target), json_file, ensure_ascii=False)\n","    json_file.close()\n","    \n","\n","def find_reviews (target_name, num_max_targets, num_max_reviews):        \n","  file_existing_items = target_name + '_Items.json'\n","  results_file = target_name + '_Results.json'\n","  \n","  if (os.path.isfile(file_existing_items) == False):\n","    arquivo = open(file_existing_items, \"w+\")\n","    arquivo.close()\n","  \n","  if (os.path.isfile(results_file) == False):\n","    arquivo = open(results_file, \"w+\")\n","    arquivo.close()\n","  \n","  # Items.json vazio\n","  if os.stat(file_existing_items).st_size == 0:\n","    extract_empty_items(num_max_reviews, num_max_targets,\n","                        file_existing_items, results_file, target_name)\n","  # Items.json preenchido\n","  else:\n","    extract_existing_items(num_max_reviews, num_max_targets,\n","                            file_existing_items, results_file, target_name)\n","  \n","  print(\"--FIM--\")\n","\n","\n","# Mapeamento #\n","# Buscas disponíveis: livros, eletronicos e brinquedos\n","# Primeiro índice = target\n","# Segundo índice = Um review contendo o texto e avaliação por estrela\n","# Terceiro índice = autor do review[0],  estrela[1], data[2], review[3],\n","# print(reviews_and_stars[0][0][3])\n","\n","# Busca por 50 livros contendo 20 reviews de cada no máximo\n","# Ex.: find_reviews (livros, 50, 20)\n","# Número de reviews deve ser múltiplo de 10\n","\n","find_reviews('livros',20,50)\n","\n","find_reviews('eletronicos',20,50)\n","\n","find_reviews('brinquedos',20,50)"]},{"cell_type":"code","source":[""],"metadata":{"id":"3Vls4MIdvcBy"},"execution_count":null,"outputs":[]}]}