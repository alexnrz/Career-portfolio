{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"analysis_review.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"19xCBfofNemxptToPYEg0QommH2kPyDcy","authorship_tag":"ABX9TyOI6UEIZ+aJpqCsG4P9oVlI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\"\"\"\n","Created on Mon Oct 2021\n","\n","@author: Alex Nascimento Rodrigues\n","\"\"\""],"metadata":{"id":"t88Gs1CSvqs0"}},{"cell_type":"code","source":["!pip install nltk\n","!pip install pyreadr"],"metadata":{"id":"j3rKAD8X4CkQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657906833044,"user_tz":180,"elapsed":6277,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}},"outputId":"8af94965-1da9-4ef4-dea7-93211525700f"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyreadr in /usr/local/lib/python3.7/dist-packages (0.4.6)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyreadr) (1.3.5)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.15.0)\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from unicodedata import normalize \n","import pyreadr, csv, string, os, nltk, json, re, statistics, pandas as pd, numpy as np\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"id":"G5azaFrN3uiQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657906833045,"user_tz":180,"elapsed":13,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}},"outputId":"0076dc90-2657-4b4b-d31b-87b7476b5d8c"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# Criação de um super léxico baseados em diversos léxicos #\n","\n","# Foram utilizados os Léxicos Vader_lexicon_ptbr, SentiLex-lem-PT02, Oplexicon_v3.0\n","\n","#https://github.com/rafjaa/LeIA/tree/master/lexicons\n","#https://github.com/sillasgonzaga/lexiconPT/blob/master/data-raw/SentiLex-lem-PT02.txt\n","#https://www.inf.pucrs.br/linatural/wordpress/recursos-e-ferramentas/oplexicon/\n"],"metadata":{"id":"YQQxU8AQKWA_","executionInfo":{"status":"ok","timestamp":1657906833045,"user_tz":180,"elapsed":4,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# Remove Stopwords\n","stop = stopwords.words('portuguese')\n","\n","# Clean and Remove special characters\n","def clean(text):\n","  text = re.sub(r'#\" ','', text)\n","  text = re.sub(r'[0-9]+', '', text)\n","  text = text.translate(str.maketrans('', '', string.punctuation))\n","  return text\n","\n","def remover_acentos(text, codif='utf-8'):\n","  return normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n","\n","# Remove incorrect text\n","def clean_incorrect(text):\n","  text = re.sub(r'video player is loadingplay videoplaymutecurrent time duration loaded','', text)\n","  text = re.sub(r'stream type liveseek to live currently playing liveliveremaining time playback ratexchapterschaptersdescriptionsdescriptions','', text)\n","  text = re.sub(r'off selectedcaptionslegendas desativadas selectedaudio trackfullscreenthis is modal window','', text)\n","  return text\n","\n","# Clean Stars column\n","def cleanStars(text):\n","  text = re.sub(r',0','', text)\n","  return text\n","\n","##################\n","## Sentilex Lem ##\n","##################\n","\n","sentiLexPath = \"drive/MyDrive/TCC-Alex-Rodrigues/lexicos/sentilex_pt_02.txt\"\n","\n","sentiLexDict = pd.read_csv(sentiLexPath, sep=\";\", header=None, on_bad_lines='skip', names=[\"term\", \"tg\", \"polarity\", \"anot\"])\n","\n","# Clean lexicon #\n","\n","def convertValueToFloat(dic):\n","  for key, val in dic.items():                                              \n","    dic[key] = float(val)\n","  return dic\n","              \n","def cleanTerm(text):\n","  text = text.split('.', 1)[0]\n","  return text\n","\n","sentiLexDict['term'] = sentiLexDict['term'].apply(cleanTerm)\n","sentiLexDict['term'] = sentiLexDict['term'].apply(remover_acentos)\n","\n","def cleanPolarity(text):\n","  text = re.sub(r'POL:N0=','', text)  \n","  text = re.sub(r'POL:N1=','', text)\n","  return text\n","\n","sentiLexDict['polarity'] = sentiLexDict['polarity'].apply(cleanPolarity)\n","sentiLexDict = sentiLexDict.set_index('term')['polarity'].to_dict()\n","sentiLexDict = convertValueToFloat(sentiLexDict)\n","      \n","##################\n","## OpLexicon v3 ##\n","##################\n","\n","opLexiconPath = 'drive/MyDrive/TCC-Alex-Rodrigues/lexicos/oplexicon_v3.0.rda'\n","opLexiconDict = pyreadr.read_r(opLexiconPath)\n","opLexiconDict = opLexiconDict['oplexicon_v3.0']\n","opLexiconDict.pop('polarity_revision')\n","opLexiconDict.pop('type')\n","\n","opLexiconDict['term'] = opLexiconDict['term'].apply(clean)\n","opLexiconDict = opLexiconDict.set_index('term')['polarity'].to_dict()\n","opLexiconDict = convertValueToFloat(opLexiconDict)\n","\n","###################\n","## Vader Lexicon ##\n","###################\n","vaderLexiconPTPath = 'drive/MyDrive/TCC-Alex-Rodrigues/lexicos/vader_lexicon_ptbr.txt'\n","\n","with open(vaderLexiconPTPath, encoding='utf-8') as f:\n","    lexicon_full_filepath = f.read()\n","\n","vaderDict = {}\n","\n","for line in lexicon_full_filepath.split('\\n'):\n","    if len(line) < 1:\n","        continue\n","    (word, measure) = line.strip().split('\\t')[0:2]\n","    vaderDict[word] = float(measure)\n","\n","vaderDict = convertValueToFloat(vaderDict)\n","vaderDict.update(sentiLexDict)\n","vaderDict.update(opLexiconDict)\n","\n","lexicos_negativos = 0\n","lexicos_neutros = 0\n","lexicos_positivos = 0\n","\n","def normalizeNotes(dic):\n","  for key, val in dic.items():\n","    if (val < 0):\n","      dic[key] = -1\n","    elif (val > 0):\n","      dic[key] = 1\n","  return dic\n","\n","vaderDict = normalizeNotes(vaderDict)\n","\n","for key, val in vaderDict.items():\n","  if (val == -1):\n","    vaderDict[key] = -1\n","    lexicos_negativos+=1\n","  elif (val == 0):\n","    lexicos_neutros+=1\n","    vaderDict[key] = 0\n","  elif (val == 1):\n","    lexicos_positivos+=1\n","    vaderDict[key] = 1\n","  else:\n","    print(\"Consertar = \" + str(key) + \":\" + str(val))\n","\n","print(\"Negativos: \" + str(lexicos_negativos))\n","print(\"Neutros: \" + str(lexicos_neutros))\n","print(\"Positivos: \" + str(lexicos_positivos))\n","\n","completeDict = vaderDict\n","\n","def review_analyzer(review):\n","  tokens = word_tokenize(review)\n","  noteDict = {'neg': 0,\n","              'neu': 0,\n","              'pos': 0}\n","  for token in tokens:\n","    if (token in completeDict.keys()):\n","      if (completeDict[token] == -1):\n","        noteDict['neg'] = noteDict['neg'] + 1\n","      elif (completeDict[token] == 0):\n","        noteDict['neu'] = noteDict['neu'] + 1\n","      elif (completeDict[token] == 1):\n","        noteDict['pos'] = noteDict['pos'] + 1\n","    # Token não está presente no léxico\n","    else:\n","      noteDict['neu'] = noteDict['neu'] + 1\n","  return noteDict\n","\n","\n","def calculate_stars_weight(stars):\n","\n","  list_neg = []\n","  list_neu = []\n","  list_pos = []\n","\n","  for star in stars:\n","    if(int(star) > 3):\n","      list_neg.append(0)\n","      list_neu.append(0)\n","      list_pos.append(1)\n","      \n","    elif (int(star) == 3):\n","      list_neg.append(0)\n","      list_neu.append(1)\n","      list_pos.append(0)\n","    else:\n","      list_neg.append(1)\n","      list_neu.append(0)\n","      list_pos.append(0)\n","\n","  return list_neg, list_neu, list_pos\n","\n","def calculateFinalResult(values):\n","  negative = values['Negative']\n","  positive = values['Positive']\n","  \n","  star_neg = values['Star_Neg']\n","  star_pos = values['Star_Pos']\n","\n","  list_final_result = []\n","  \n","  for i in range(len(negative)):\n","\n","    negative[i] = negative[i] + (0.3 * negative[i] * star_neg[i])\n","    positive[i] = positive[i] + (0.3 * positive[i] * star_neg[i])\n","\n","    if(negative[i] > positive[i]):\n","      list_final_result.append(\"Negative\")\n","    elif(negative[i] < positive[i]):\n","      list_final_result.append(\"Positive\")\n","    else:\n","      list_final_result.append(\"Neutral\")        \n","  \n","  return list_final_result\n","  \n","def percentage(part, whole):  \n","  if (whole == 0):\n","    return str(0) + \"%\"\n","  percentage = 100 * float(part)/float(whole)\n","  return str(round(percentage,2)) + \"%\"\n","\n","def set_percentage_with_neutral(values):\n","\n","  negative = values['Negative']\n","  neutral = values['Neutral']\n","  positive = values['Positive']\n","\n","  list_neg = []\n","  list_neu = []\n","  list_pos = []\n","\n","  for i in range(len(negative)):\n","    total = int(negative[i]+neutral[i]+positive[i])\n","    list_neg.append(percentage(negative[i], total))\n","    list_neu.append(percentage(neutral[i], total))\n","    list_pos.append(percentage(positive[i], total))\n","  return list_neg, list_neu, list_pos\n","\n","def set_percentage(values):\n","  negative = values['Negative']\n","  positive = values['Positive']\n","  list_neg_pos = []\n","\n","  for i in range(len(negative)):\n","    total = int(negative[i]+positive[i])\n","    list_neg_pos.append(percentage(negative[i], total) + ' / ' + percentage(positive[i], total))\n","  return list_neg_pos"],"metadata":{"id":"nazmW-kHCxd3","executionInfo":{"status":"ok","timestamp":1657906833861,"user_tz":180,"elapsed":820,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"241d616c-8150-4d9a-9396-dcc11c78fa76"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Negativos: 18054\n","Neutros: 8981\n","Positivos: 10740\n"]}]},{"cell_type":"code","source":["#######################################\n","### Limpeza do Dataset para análise ###\n","#######################################\n","\n","with open('drive/MyDrive/TCC-Alex-Rodrigues/extract_reviews/eletronicos_Results.json', encoding='utf-8') as fh:\n","    data_eletronicos = json.load(fh)\n","\n","data_all_categories = data_eletronicos\n","\n","with open('drive/MyDrive/TCC-Alex-Rodrigues/extract_reviews/brinquedos_Results.json', encoding='utf-8') as fh:\n","    data_brinquedos = json.load(fh)\n","    \n","data_all_categories.update(data_brinquedos)\n","\n","with open('drive/MyDrive/TCC-Alex-Rodrigues/extract_reviews/livros_Results.json', encoding='utf-8') as fh:\n","    data_livros = json.load(fh)\n","\n","data_all_categories.update(data_livros)\n","\n","# Products name\n","productsName = list(data_all_categories.keys())\n","\n","df_complete = pd.DataFrame()\n","dic_complete = {}\n","\n","cont_errors = 0\n","\n","for product in productsName:\n","  try:\n","    #print(\"-----------PRODUTO ATUAL------------ \" + str(product))\n","    raw_reviews = data_all_categories.get(product)\n","    new_df = pd.DataFrame(raw_reviews, columns=['Name', 'Stars', 'Date', 'Review'])\n","    new_df.dropna(inplace = True)\n","    \n","    new_df['Raw_Review'] = new_df['Review']\n","    \n","    new_df['Item'] = product\n","    # Clean and Remove special characters\n","    new_df['Processed_Review'] = new_df['Review'].apply(clean)\n","    # Lowercase all reviews\n","    new_df['Processed_Review'] = new_df['Processed_Review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n","    # Remove Stopwords\n","    new_df['Processed_Review'] = new_df['Processed_Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","    # Remoção da acentuação\n","    new_df['Processed_Review'] = new_df['Processed_Review'].apply(remover_acentos)\n","    # Clean Stars\n","    new_df['Stars'] = new_df['Stars'].apply(cleanStars)\n","    # Clean incorrect text\n","    new_df['Processed_Review'] = new_df['Processed_Review'].apply(clean_incorrect)\n","\n","    new_df['Scores'] = new_df['Processed_Review'].apply(review_analyzer)\n","\n","    new_df['Negative']  = new_df['Scores'].apply(lambda score_dict: score_dict['neg'])\n","    new_df['Neutral']  = new_df['Scores'].apply(lambda score_dict: score_dict['neu'])\n","    new_df['Positive']  = new_df['Scores'].apply(lambda score_dict: score_dict['pos'])\n","    del new_df['Scores']\n","\n","    new_df['Star_Neg'], new_df['Star_Neu'], new_df['Star_Pos'] = calculate_stars_weight(new_df['Stars'])\n","        \n","    new_df['Final_Result'] = calculateFinalResult(new_df[['Negative', 'Positive','Star_Neg','Star_Neu', 'Star_Pos']])\n","    new_df['Neg(%)'], new_df['Neu(%)'], new_df['Pos(%)'] = set_percentage_with_neutral(new_df[['Negative','Neutral','Positive']])\n","    new_df['Neg/Pos(%)'] = set_percentage(new_df[['Negative','Positive']])  \n","\n","    new_df = new_df.reindex(['Item', 'Name', 'Date', 'Raw_Review', 'Processed_Review', 'Stars', 'Star_Neg', 'Star_Neu', 'Star_Pos',\n","                             'Negative', 'Neutral', 'Positive', 'Neg(%)', 'Neu(%)', 'Pos(%)', 'Neg/Pos(%)', 'Final_Result'], axis=1)    \n","    \n","    df_complete = pd.concat([df_complete, new_df])\n","    \n","  except:\n","    print(\"-----Erro ao analisar o produto----- \" + str(product))    \n","    cont_errors+=1\n","    continue\n","\n","df_complete['Processed_Review'].replace('', np.nan, inplace=True)\n","df_complete.dropna(subset=['Processed_Review'], inplace=True)\n","\n","print(\"Produtos não avaliados \" + str(cont_errors) + \" de \" + str(len(productsName)))\n","\n","#df_complete.to_csv('drive/My Drive/TCC-NEW/Reviews-Dataset.csv', sep=';', encoding='utf-8', header=True, index=False)\n","\n","df_complete.to_excel('drive/MyDrive/TCC-Alex-Rodrigues/Reviews-Dataset.xlsx', encoding='utf-8', header=True, index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQ397CptOs8A","executionInfo":{"status":"ok","timestamp":1657906960930,"user_tz":180,"elapsed":127083,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}},"outputId":"dde40f8b-32ee-4004-a422-eef800e32d2f"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:177: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:178: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["Produtos não avaliados 0 de 2999\n"]}]},{"cell_type":"code","source":["\n","print('Negative Results: ',len(df_complete.loc[df_complete['Final_Result'] == 'Negative']))\n","print('Neutral Results: ',len(df_complete.loc[df_complete['Final_Result'] == 'Neutral']))\n","print('Positive Results: ',len(df_complete.loc[df_complete['Final_Result'] == 'Positive']))\n","\n","print('Stars Average: ', round(statistics.mean(df_complete['Stars'].astype(int)),2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANYDF0Eews9L","executionInfo":{"status":"ok","timestamp":1657906960931,"user_tz":180,"elapsed":18,"user":{"displayName":"Alex Nascimento","userId":"06103239724283764527"}},"outputId":"3a037128-7be5-4b31-91ec-3ae5f9b0997e"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Negative Results:  9449\n","Neutral Results:  8497\n","Positive Results:  38848\n","Stars Average:  4.4\n"]}]}]}